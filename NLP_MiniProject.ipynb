{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP MiniProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHO6Gv8Qgbag"
      },
      "source": [
        "# Transliteration from English to Marathi\n",
        "## Using Encoder - Decoder with Attention Layers\n",
        "\n",
        "### Team: (BCA 1 - Group 2)\n",
        "1. Manas Acharya (01, 172066)\n",
        "2. Ritika Bhole (07, 172089)\n",
        "3. Sahil Nirkhe (12, 172099)\n",
        "4. Sanket Dalvi (13, 172104) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2hjYdOTE0XG"
      },
      "source": [
        "## Acquire Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAagw45p2oBt",
        "outputId": "657bb0ae-385d-4d94-9add-81e5b33a6a86"
      },
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-16 10:17:18--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.122.128, 172.253.63.128, 142.250.31.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.122.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  34.4MB/s    in 17s     \n",
            "\n",
            "2021-05-16 10:17:35 (113 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tu0keGVBA5u"
      },
      "source": [
        "!tar -xf dakshina_dataset_v1.0.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSdCud5VBmKa",
        "outputId": "6dc82242-599b-414f-e147-fd5aa98cde21"
      },
      "source": [
        "!head /content/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "swar\n",
            "19 vya shatakamadhye jevha British East India Companyne Bharatacha sampurna prashaskiya karbhar aplya hatat ghetla tevha tya prakriyemadhye ek navin asa nokardar ingraji bolnara madhyam varga udayaas aala.\n",
            "Mandirawar shikharaaivaji ekavar ek ashi Shivlinge ahet.\n",
            "chal dhar pakad\n",
            "sadharan teen varshantun ekda ha nisarga chamatkar pahayla milto.\n",
            "sakshtva mhanaje pahane athava janane.\n",
            "hajaro varshanpasun mahaan vyaktinchya murti ghadavlya jaat ahet.\n",
            "tyanna mahit hote ki Munich akramankartyanchya sutkesathi Germanyla dahashatvadi karyancha saamna karava lagel.\n",
            "cheda lahana asalyamule vyayama va saundryachya drishtikonatun uttam asate.\n",
            "Kanitkar yanni anek aitihasik pustakanche lekhan kele hote.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYKIBDlgByih",
        "outputId": "abdb3631-fdb7-42dc-9698-f4f7be8b2120"
      },
      "source": [
        "!head /content/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "स्वर\n",
            "१९ व्या शतकामध्ये जेव्हा ब्रिटिश ईस्ट इंडिया कंपनीने भारताचा संपूर्ण प्रशासकीय कारभार आपल्या हातात घेतला तेव्हा त्या प्रक्रियेमध्ये एक नवीन असा नोकरदार इंग्रजी बोलणारा मध्यम वर्ग उदयास आला.\n",
            "मंदिरावर शिखराऐवजी एकावर एक अशी शिवलिंगे आहेत.\n",
            "चल धर पकड\n",
            "साधारण तीन वर्षांतून एकदा हा निसर्ग चमत्कार पहायला मिळतो.\n",
            "साक्षत्व म्हणजे पाहणे अथवा जाणणे.\n",
            "हजारो वर्षांपासून महान व्यक्तींच्या मूर्ती घडवल्या जात आहेत.\n",
            "त्यांना माहीत होते की म्युनिच आक्रमणकर्त्यांच्या सुटकेसाठी जर्मनीला दहशतवादी कार्यांचा सामना करावा लागेल.\n",
            "छेद लहान असल्यामुळे व्यायाम व सौंदर्याच्या दृष्टिकोनातून उत्तम असते.\n",
            "कानिटकर यांनी अनेक ऐतिहासिक पुस्तकांचे लेखन केले होते.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkmoTeo4E8TC"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zacZF3WVEY6W",
        "outputId": "5a4ebf6e-97bb-4863-bcf9-eeea5474141f"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdMK6n7xFgg3"
      },
      "source": [
        "## Text Processing helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DacvI84HFlvX"
      },
      "source": [
        "### Devnagiri alphabets list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM6oxhwMFF3j",
        "outputId": "0df51481-ca59-4d23-c802-6b802add3cf0"
      },
      "source": [
        "marathi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "marathi_alphabet_size = len(marathi_alphabets)\n",
        "\n",
        "marathi_alpha2index = {'<start>': 0,'<end>' : 1}\n",
        "for index, alpha in enumerate(marathi_alphabets):\n",
        "    marathi_alpha2index[alpha] = index+1\n",
        "\n",
        "# pprint(marathi_alpha2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<end>': 1,\n",
            " '<start>': 0,\n",
            " 'ऀ': 1,\n",
            " 'ँ': 2,\n",
            " 'ं': 3,\n",
            " 'ः': 4,\n",
            " 'ऄ': 5,\n",
            " 'अ': 6,\n",
            " 'आ': 7,\n",
            " 'इ': 8,\n",
            " 'ई': 9,\n",
            " 'उ': 10,\n",
            " 'ऊ': 11,\n",
            " 'ऋ': 12,\n",
            " 'ऌ': 13,\n",
            " 'ऍ': 14,\n",
            " 'ऎ': 15,\n",
            " 'ए': 16,\n",
            " 'ऐ': 17,\n",
            " 'ऑ': 18,\n",
            " 'ऒ': 19,\n",
            " 'ओ': 20,\n",
            " 'औ': 21,\n",
            " 'क': 22,\n",
            " 'ख': 23,\n",
            " 'ग': 24,\n",
            " 'घ': 25,\n",
            " 'ङ': 26,\n",
            " 'च': 27,\n",
            " 'छ': 28,\n",
            " 'ज': 29,\n",
            " 'झ': 30,\n",
            " 'ञ': 31,\n",
            " 'ट': 32,\n",
            " 'ठ': 33,\n",
            " 'ड': 34,\n",
            " 'ढ': 35,\n",
            " 'ण': 36,\n",
            " 'त': 37,\n",
            " 'थ': 38,\n",
            " 'द': 39,\n",
            " 'ध': 40,\n",
            " 'न': 41,\n",
            " 'ऩ': 42,\n",
            " 'प': 43,\n",
            " 'फ': 44,\n",
            " 'ब': 45,\n",
            " 'भ': 46,\n",
            " 'म': 47,\n",
            " 'य': 48,\n",
            " 'र': 49,\n",
            " 'ऱ': 50,\n",
            " 'ल': 51,\n",
            " 'ळ': 52,\n",
            " 'ऴ': 53,\n",
            " 'व': 54,\n",
            " 'श': 55,\n",
            " 'ष': 56,\n",
            " 'स': 57,\n",
            " 'ह': 58,\n",
            " 'ऺ': 59,\n",
            " 'ऻ': 60,\n",
            " '़': 61,\n",
            " 'ऽ': 62,\n",
            " 'ा': 63,\n",
            " 'ि': 64,\n",
            " 'ी': 65,\n",
            " 'ु': 66,\n",
            " 'ू': 67,\n",
            " 'ृ': 68,\n",
            " 'ॄ': 69,\n",
            " 'ॅ': 70,\n",
            " 'ॆ': 71,\n",
            " 'े': 72,\n",
            " 'ै': 73,\n",
            " 'ॉ': 74,\n",
            " 'ॊ': 75,\n",
            " 'ो': 76,\n",
            " 'ौ': 77,\n",
            " '्': 78,\n",
            " 'ॎ': 79,\n",
            " 'ॏ': 80,\n",
            " 'ॐ': 81,\n",
            " '॑': 82,\n",
            " '॒': 83,\n",
            " '॓': 84,\n",
            " '॔': 85,\n",
            " 'ॕ': 86,\n",
            " 'ॖ': 87,\n",
            " 'ॗ': 88,\n",
            " 'क़': 89,\n",
            " 'ख़': 90,\n",
            " 'ग़': 91,\n",
            " 'ज़': 92,\n",
            " 'ड़': 93,\n",
            " 'ढ़': 94,\n",
            " 'फ़': 95,\n",
            " 'य़': 96,\n",
            " 'ॠ': 97,\n",
            " 'ॡ': 98,\n",
            " 'ॢ': 99,\n",
            " 'ॣ': 100,\n",
            " '।': 101,\n",
            " '॥': 102,\n",
            " '०': 103,\n",
            " '१': 104,\n",
            " '२': 105,\n",
            " '३': 106,\n",
            " '४': 107,\n",
            " '५': 108,\n",
            " '६': 109,\n",
            " '७': 110,\n",
            " '८': 111,\n",
            " '९': 112,\n",
            " '॰': 113,\n",
            " 'ॱ': 114,\n",
            " 'ॲ': 115,\n",
            " 'ॳ': 116,\n",
            " 'ॴ': 117,\n",
            " 'ॵ': 118,\n",
            " 'ॶ': 119,\n",
            " 'ॷ': 120,\n",
            " 'ॸ': 121,\n",
            " 'ॹ': 122,\n",
            " 'ॺ': 123,\n",
            " 'ॻ': 124,\n",
            " 'ॼ': 125,\n",
            " 'ॽ': 126,\n",
            " 'ॾ': 127,\n",
            " 'ॿ': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fiO_oUZfMVp"
      },
      "source": [
        "### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFqwdaYcF5h9"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s))\n",
        "        # if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence_english(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = w.replace('-', ' ').replace(',', ' ')\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '@' + w + '#'\n",
        "    return w.split()\n",
        "\n",
        "def preprocess_sentence_marathi(w):\n",
        "    w = unicode_to_ascii(w.strip())\n",
        "    w = w.replace('-', ' ').replace(',', ' ')\n",
        "    cleaned_line = ''\n",
        "    for char in w:\n",
        "        if char in marathi_alpha2index or char == ' ':\n",
        "            cleaned_line += char\n",
        "    cleaned_line = cleaned_line.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    cleaned_line = '@' + cleaned_line + '#'\n",
        "    return cleaned_line.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iLTvfTIfWA0"
      },
      "source": [
        "### Creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0rqRAomUklD"
      },
      "source": [
        "def create_dataset(data):\n",
        "    lang1_words = []\n",
        "    lang2_words = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        try:\n",
        "            wordlist1 = preprocess_sentence_english(row['en']) # clean English words.\n",
        "            wordlist2 = preprocess_sentence_marathi(row['mr']) # clean marathi words.\n",
        "            if len(wordlist1) != len(wordlist2):\n",
        "                print('Skipping: ', row['en'], ' - ', row['mr'])\n",
        "                continue\n",
        "            for word in wordlist1:\n",
        "                lang1_words.append(word)\n",
        "            for word in wordlist2:\n",
        "                lang2_words.append(word)\n",
        "        except Exception as e:\n",
        "            print(f\"Found error at {row}\")\n",
        "    return [lang1_words,lang2_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gxwf93FUznz"
      },
      "source": [
        "#### Train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu9_WUnQGoFK"
      },
      "source": [
        "train = pd.read_csv(\"/content/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\", sep=\"\\t\", names = ['mr', 'en', 'lex'], header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JM20XrNYD-3"
      },
      "source": [
        "train.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBXnQU2UH52e"
      },
      "source": [
        "train_data = create_dataset(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx24ur-kU93y",
        "outputId": "916bf1cd-e6d5-4b45-96f4-1fcad7dc491e"
      },
      "source": [
        "for i in range(50):\n",
        "    print(f\"{train_data[0][-i]} => {train_data[1][-i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@angry# => @अँग्री#\n",
            "@yalan# => @ॲलन#\n",
            "@elan# => @ॲलन#\n",
            "@allan# => @ॲलन#\n",
            "@alana# => @ॲलन#\n",
            "@alan# => @ॲलन#\n",
            "@ailan# => @ॲलन#\n",
            "@hrudayat# => @ह्रदयात#\n",
            "@hyuustana# => @ह्यूस्टन#\n",
            "@hyustana# => @ह्यूस्टन#\n",
            "@huustun# => @ह्यूस्टन#\n",
            "@huusetun# => @ह्यूस्टन#\n",
            "@hustun# => @ह्यूस्टन#\n",
            "@houuston# => @ह्यूस्टन#\n",
            "@houuseton# => @ह्यूस्टन#\n",
            "@houston# => @ह्यूस्टन#\n",
            "@houseton# => @ह्यूस्टन#\n",
            "@hoston# => @ह्यूस्टन#\n",
            "@hayauuston# => @ह्यूस्टन#\n",
            "@hayauustan# => @ह्यूस्टन#\n",
            "@hayaustun# => @ह्यूस्टन#\n",
            "@hayauston# => @ह्यूस्टन#\n",
            "@hayaustan# => @ह्यूस्टन#\n",
            "@haustan# => @ह्यूस्टन#\n",
            "@hausetan# => @ह्यूस्टन#\n",
            "@hastan# => @ह्यूस्टन#\n",
            "@hyooman# => @ह्यूमन#\n",
            "@human# => @ह्यूमन#\n",
            "@hyustan# => @ह्युस्टन#\n",
            "@houston# => @ह्युस्टन#\n",
            "@hyasarakhya# => @ह्यासारख्या#\n",
            "@hyasarakha# => @ह्यासारख्या#\n",
            "@hyasathi# => @ह्यासाठी#\n",
            "@hyasathee# => @ह्यासाठी#\n",
            "@hyas# => @ह्यास#\n",
            "@hyavarun# => @ह्यावरून#\n",
            "@hyavaroon# => @ह्यावरून#\n",
            "@hyavarun# => @ह्यावरुन#\n",
            "@hyavaroon# => @ह्यावरुन#\n",
            "@hyavar# => @ह्यावर#\n",
            "@hyala# => @ह्याला#\n",
            "@hyamule# => @ह्यामुळे#\n",
            "@hyamage# => @ह्यामध्ये#\n",
            "@hyamadhye# => @ह्यामध्ये#\n",
            "@hyababat# => @ह्याबाबत#\n",
            "@hyane# => @ह्याने#\n",
            "@hyatun# => @ह्यातून#\n",
            "@hyatoon# => @ह्यातून#\n",
            "@hyatalya# => @ह्यातल्या#\n",
            "@hyatale# => @ह्यातले#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYoTv-ZnZlp9",
        "outputId": "705d9d90-77f6-4e17-8bcc-fb45fde05a34"
      },
      "source": [
        "len(train_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ7XNy_naubq"
      },
      "source": [
        "#### Validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Skk1HBa2nh"
      },
      "source": [
        "dev = pd.read_csv(\"/content/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\", sep=\"\\t\", names = ['mr', 'en', 'lex'], header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8CD0hSLa2nj"
      },
      "source": [
        "dev.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O32RFwk3a2nk"
      },
      "source": [
        "dev_data = create_dataset(dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3QWWxl3a2nl",
        "outputId": "ecbcac8a-a869-4fdf-d051-b48d0c54c748"
      },
      "source": [
        "for i in range(10):\n",
        "    print(f\"{dev_data[0][i]} => {dev_data[1][i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@aendarsanla# => @अँडरसनला#\n",
            "@andersonla# => @अँडरसनला#\n",
            "@andrew# => @अँड्र्यू#\n",
            "@ankaleekar# => @अंकलीकर#\n",
            "@ankalikar# => @अंकलीकर#\n",
            "@ankalivar# => @अंकलीकर#\n",
            "@anchal# => @अंचल#\n",
            "@antarctica# => @अंटार्क्टिका#\n",
            "@antarctika# => @अंटार्क्टिका#\n",
            "@antarctiqa# => @अंटार्क्टिका#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Eh9h6Fa2nn",
        "outputId": "5ff0fdf2-d5df-4ba6-bb7d-061bbaf5fc9c"
      },
      "source": [
        "len(dev_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5658"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYIvDMCIbSH8"
      },
      "source": [
        "## Vocabulary Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbBUd3AzbaGs"
      },
      "source": [
        "class WordIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      for l in phrase:\n",
        "        self.vocab.update(l)\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hwcs1dtbfUt"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(pairs, max_length_inp=None, max_length_tar=None):\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = WordIndex(pairs[0])\n",
        "    targ_lang = WordIndex(pairs[1])\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    # English words\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in en] for en in pairs[0]]\n",
        "    \n",
        "    # marathi words\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in mr] for mr in pairs[1]]\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    if max_length_inp is None or max_length_tar is None:\n",
        "        max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyVBOnU_b9YR"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.idx2word[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCr-OUUKeivU"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tf8UEXGhKpQ"
      },
      "source": [
        "### Train and val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ_q3G6CcmBj"
      },
      "source": [
        "input_tensor_train, target_tensor_train, inp_lang, targ_lang, max_length_inp, max_length_tar = load_dataset(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeftddPee8y0"
      },
      "source": [
        "input_tensor_val, target_tensor_val, inp_lang, targ_lang, max_length_inp, max_length_tar = load_dataset(dev_data, max_length_inp, max_length_tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH89Fvk7gxpq",
        "outputId": "296537c7-57bd-4ed0-9908-625226643849"
      },
      "source": [
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56300, 56300, 5658, 5658)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbc7I61LjI3R"
      },
      "source": [
        "Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dKSkhTfldgC"
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar = load_dataset(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyKTCfmhi4DL"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0usULljjHuG",
        "outputId": "7c645e64-33d7-4c4b-a0e4-9da8d8be822f"
      },
      "source": [
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53485, 53485, 2815, 2815)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkNkq9H8hOdN"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsjPptm4hHqZ"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)+1\n",
        "vocab_tar_size = len(targ_lang.word2idx)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbNqFn42hUiL"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r75lB4ezhYg2",
        "outputId": "2e894915-8b1f-493b-ed6d-8d74c82f2a0c"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 25]), TensorShape([128, 22]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNFVB99HhdO2"
      },
      "source": [
        "### Encoder - Decoder with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR3OBLxvhbJB"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOyHMSJxhkaV",
        "outputId": "877129ce-0200-4917-fbb3-2e724e91a1e1"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 25, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (128, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b9hYAW3hoZB"
      },
      "source": [
        "class Attention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu_wH8lZh0uH",
        "outputId": "574a5afa-b82f-4a23-d662-7da247ec6856"
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (128, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (128, 25, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJZD4le3h4qO"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IquGImIaiBey",
        "outputId": "0313a3a3-c3b9-470d-a346-b8040022b350"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((128, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 68)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vQqb7chiDoH"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiK3m6H1iGf4"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FiFaPGKiLi5"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T_ZPZR9iLLS"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['@']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WchhVqCbr7wM",
        "outputId": "8f1fd35e-0fff-4771-ceec-f9c8302e276b"
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy7ecKGhrqRM",
        "outputId": "1ab54899-3333-4a13-e6e7-7a3ebd0c1c18"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.5387\n",
            "Epoch 1 Batch 100 Loss 0.9606\n",
            "Epoch 1 Batch 200 Loss 0.4905\n",
            "Epoch 1 Batch 300 Loss 0.2433\n",
            "Epoch 1 Batch 400 Loss 0.2102\n",
            "Epoch 1 Loss 0.6020\n",
            "Time taken for 1 epoch 121.74392604827881 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2115\n",
            "Epoch 2 Batch 100 Loss 0.1817\n",
            "Epoch 2 Batch 200 Loss 0.1676\n",
            "Epoch 2 Batch 300 Loss 0.1430\n",
            "Epoch 2 Batch 400 Loss 0.1513\n",
            "Epoch 2 Loss 0.1639\n",
            "Time taken for 1 epoch 98.15489220619202 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1388\n",
            "Epoch 3 Batch 100 Loss 0.1227\n",
            "Epoch 3 Batch 200 Loss 0.1073\n",
            "Epoch 3 Batch 300 Loss 0.1036\n",
            "Epoch 3 Batch 400 Loss 0.1016\n",
            "Epoch 3 Loss 0.1175\n",
            "Time taken for 1 epoch 97.7836012840271 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1192\n",
            "Epoch 4 Batch 100 Loss 0.0950\n",
            "Epoch 4 Batch 200 Loss 0.1280\n",
            "Epoch 4 Batch 300 Loss 0.1045\n",
            "Epoch 4 Batch 400 Loss 0.1074\n",
            "Epoch 4 Loss 0.1007\n",
            "Time taken for 1 epoch 97.88889980316162 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0833\n",
            "Epoch 5 Batch 100 Loss 0.0823\n",
            "Epoch 5 Batch 200 Loss 0.0788\n",
            "Epoch 5 Batch 300 Loss 0.0736\n",
            "Epoch 5 Batch 400 Loss 0.0679\n",
            "Epoch 5 Loss 0.0764\n",
            "Time taken for 1 epoch 97.5856020450592 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0528\n",
            "Epoch 6 Batch 100 Loss 0.0554\n",
            "Epoch 6 Batch 200 Loss 0.0522\n",
            "Epoch 6 Batch 300 Loss 0.0836\n",
            "Epoch 6 Batch 400 Loss 0.0500\n",
            "Epoch 6 Loss 0.0591\n",
            "Time taken for 1 epoch 98.03736853599548 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0388\n",
            "Epoch 7 Batch 100 Loss 0.0475\n",
            "Epoch 7 Batch 200 Loss 0.0401\n",
            "Epoch 7 Batch 300 Loss 0.0412\n",
            "Epoch 7 Batch 400 Loss 0.0718\n",
            "Epoch 7 Loss 0.0468\n",
            "Time taken for 1 epoch 97.6844437122345 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0391\n",
            "Epoch 8 Batch 100 Loss 0.0386\n",
            "Epoch 8 Batch 200 Loss 0.0601\n",
            "Epoch 8 Batch 300 Loss 0.0501\n",
            "Epoch 8 Batch 400 Loss 0.0371\n",
            "Epoch 8 Loss 0.0475\n",
            "Time taken for 1 epoch 98.08061456680298 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0226\n",
            "Epoch 9 Batch 100 Loss 0.0289\n",
            "Epoch 9 Batch 200 Loss 0.0568\n",
            "Epoch 9 Batch 300 Loss 0.0741\n",
            "Epoch 9 Batch 400 Loss 0.0708\n",
            "Epoch 9 Loss 0.0485\n",
            "Time taken for 1 epoch 97.65739750862122 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0222\n",
            "Epoch 10 Batch 100 Loss 0.0255\n",
            "Epoch 10 Batch 200 Loss 0.0396\n",
            "Epoch 10 Batch 300 Loss 0.0250\n",
            "Epoch 10 Batch 400 Loss 0.0357\n",
            "Epoch 10 Loss 0.0328\n",
            "Time taken for 1 epoch 97.83995771408081 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19hfysIgJmT"
      },
      "source": [
        "## Using the Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YTklqoBuUpT"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_tar, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence_english(sentence)[0]\n",
        "\n",
        "    inputs = [inp_lang.word2idx[i] for i in sentence]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['@']], 0)\n",
        "\n",
        "    options = []\n",
        "    for t in range(max_length_tar):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        options_for_curr_char = []\n",
        "        for idx, i in enumerate(np.argsort(predictions[0])[::-1][:2]):\n",
        "            if idx >0 and predictions[0][i] > 10:\n",
        "                options_for_curr_char.append(targ_lang.idx2word[i])\n",
        "            elif idx==0:\n",
        "                options_for_curr_char.append(targ_lang.idx2word[i])\n",
        "            # print(f\"{targ_lang.idx2word[i]} ({predictions[0][i]})\", end = \" \")\n",
        "        options.append(options_for_curr_char)\n",
        "        # print()\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "        if targ_lang.idx2word[predicted_id] == '#':\n",
        "            return options, result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return options, result, sentence, attention_plot\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "#     print(predicted_sentence)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiPJ_S9luiZy"
      },
      "source": [
        "def transliterate(sentence):\n",
        "    options, result, sentence, attention_plot = evaluate(sentence)\n",
        "    return ''.join(result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8SDbC20v-r6"
      },
      "source": [
        "def transliterate_sentence(sentence):\n",
        "    translated = \"\"\n",
        "    for i in sentence.split(\" \"):\n",
        "        translated += \" \" + transliterate(i)[:-1]\n",
        "    print(translated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK-A9ziBvbqZ",
        "outputId": "8e48489b-0dbc-4a24-c548-6f6a1c631e78"
      },
      "source": [
        "transliterate_sentence(\"maala don dole aahet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['म'], ['ा'], ['ल', 'ळ'], ['ा'], ['#']]\n",
            "[['ड', 'द'], ['ो', 'ॉ'], ['न'], ['#']]\n",
            "[['ड'], ['ो'], ['ळ', 'ल'], ['े', '#'], ['#']]\n",
            "[['आ', 'अ'], ['ह'], ['े', 'ा'], ['त', 'ट'], ['#', 'े']]\n",
            " माला डोन डोळे आहेत\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sbYSCU1zvexF",
        "outputId": "14da02b8-2bcd-42b6-bdbf-8987f8147d9b"
      },
      "source": [
        "transliterate(\"dole\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['ड'], ['ो'], ['ळ', 'ल'], ['े', '#'], ['#']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'डोळे#'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFSDHI6b8fA0"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}